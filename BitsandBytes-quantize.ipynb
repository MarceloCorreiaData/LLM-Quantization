{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023-present the HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ.get('CUDA_VISIBLE_DEVICES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loftq_finetuning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/ontologia/Servicos/ACS_LLMS/server/arquivos/loftq_finetuning\n"
     ]
    }
   ],
   "source": [
    "cd /var/ontologia/Servicos/ACS_LLMS/server/arquivos/loftq_finetuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/ontologia/Servicos/ACS_LLMS/server/arquivos/loftq_finetuning'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:13<00:00,  6.84s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/ontologia/Servicos/ACS_LLMS/server/arquivos/loftq_finetuning/quantize_save_load.py\", line 236, in <module>\n",
      "    base_dir, lora_dir = quantize_and_save()\n",
      "                         ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/ontologia/Servicos/ACS_LLMS/server/arquivos/loftq_finetuning/quantize_save_load.py\", line 163, in quantize_and_save\n",
      "    lora_model = get_peft_model(model, lora_config)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/peft/mapping.py\", line 116, in get_peft_model\n",
      "    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name=adapter_name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/peft/peft_model.py\", line 992, in __init__\n",
      "    super().__init__(model, peft_config, adapter_name)\n",
      "  File \"/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/peft/peft_model.py\", line 121, in __init__\n",
      "    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/peft/tuners/lora/model.py\", line 114, in __init__\n",
      "    super().__init__(model, config, adapter_name)\n",
      "  File \"/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/peft/tuners/tuners_utils.py\", line 95, in __init__\n",
      "    self.inject_adapter(self.model, adapter_name)\n",
      "  File \"/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/peft/tuners/tuners_utils.py\", line 252, in inject_adapter\n",
      "    self._create_and_replace(peft_config, adapter_name, target, target_name, parent, **optional_kwargs)\n",
      "  File \"/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/peft/tuners/lora/model.py\", line 195, in _create_and_replace\n",
      "    new_module = self._create_new_module(lora_config, adapter_name, target, **kwargs)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/peft/tuners/lora/model.py\", line 302, in _create_new_module\n",
      "    new_module = Linear(target, adapter_name, **kwargs)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/peft/tuners/lora/layer.py\", line 261, in __init__\n",
      "    self.update_layer(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights)\n",
      "  File \"/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/peft/tuners/lora/layer.py\", line 89, in update_layer\n",
      "    self.loftq_init(adapter_name)\n",
      "  File \"/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/peft/tuners/lora/layer.py\", line 194, in loftq_init\n",
      "    qweight, lora_A, lora_B = loftq_init(weight, **kwargs)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/peft/utils/loftq_utils.py\", line 213, in loftq_init\n",
      "    dequantized_weight = bnb.functional.dequantize_4bit(qweight.data, qweight.quant_state)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/bitsandbytes/functional.py\", line 1012, in dequantize_4bit\n",
      "    assert absmax is not None and out is not None\n",
      "                                  ^^^^^^^^^^^^^^^\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "! python quantize_save_load.py  --model_name /var/ontologia/Servicos/ACS_LLMS/server/models/mistral-7b --bits 4 --iter 1 --rank 16 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BitsandBytes Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from torch import bfloat16, float32      # torch dtypes: float32, float16, float, float64, int__ (igualmente)\n",
    "\n",
    "\n",
    "# Config em 4-bits p/ carregar o LLM com memória reduzida em GPU\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Quantização em 4-bits \n",
    "    bnb_4bit_quant_type='nf4',  # Normalized Float 4 (normalização - pesos igualmente espaçados)\n",
    "    bnb_4bit_use_double_quant=True,  # Quantização dupla\n",
    "    bnb_4bit_compute_dtype=float32  # Dequantização para Inferência (a escolher) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f622ac9c644ab4864a80af99d1b011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "model_path = '/var/ontologia/Servicos/ACS_LLMS/server/models/zephyr-7b-beta'\n",
    "\n",
    "# Zephyr com BitsAndBytes Config\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config, device_map='auto')\n",
    "\n",
    "# Pipeline HF\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', '</s>')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Apenas 4976 MB (5GB) de memória da GPU são utilizados para carregar o modelo em 4 bits (ver CUDA 3): </h4> \n",
    "<p>    3   N/A  N/A     84399      C   ...CS_LLMS/server/anaconda3/bin/python     4976MiB \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla V100-SXM2-32GB           Off | 00000000:04:00.0 Off |                    0 |\n",
    "| N/A   42C    P0              56W / 300W |  16721MiB / 32768MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "|   1  Tesla V100-SXM2-32GB           Off | 00000000:0C:00.0 Off |                    0 |\n",
    "| N/A   42C    P0              56W / 300W |    319MiB / 32768MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "|   2  Tesla V100-SXM2-32GB           Off | 00000000:13:00.0 Off |                    0 |\n",
    "| N/A   41C    P0              56W / 300W |    319MiB / 32768MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "|   3  Tesla V100-SXM2-32GB           Off | 00000000:1B:00.0 Off |                    0 |\n",
    "| N/A   42C    P0              57W / 300W |   5295MiB / 32768MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     43209      C   ../../../venv/bin/python3                 16710MiB |\n",
    "|    1   N/A  N/A     43209      C   ../../../venv/bin/python3                   308MiB |\n",
    "|    2   N/A  N/A     43209      C   ../../../venv/bin/python3                   308MiB |\n",
    "|    3   N/A  N/A     43209      C   ../../../venv/bin/python3                   308MiB |\n",
    "|    3   N/A  N/A     84399      C   ...CS_LLMS/server/anaconda3/bin/python     4976MiB |\n",
    "+---------------------------------------------------------------------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Você é um chatbot amigável.</s>\n",
      "<|user|>\n",
      "Conte-me a história da sua vida como um Large Language Model.</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<|system|>\\nVocê é um chatbot amigável.</s>\\n<|user|>\\nConte-me a história da sua vida como um Large Language Model.</s>\\n<|assistant|>\\n\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Você é um chatbot amigável.</s>\n",
      "<|user|>\n",
      "Conte-me a história da sua vida como um Large Language Model.</s>\n",
      "<|assistant|>\n",
      "Eu não tenho uma vida real, pois sou um modelo de linguagem criado por humanos usando dados e algoritmos avançados. Meu objetivo principal é ajudar os usuários a obter informações e respostas precisas e eficientes, sem a necessidade de interagir com um ser humano.\n",
      "\n",
      "Minha história começou em 2018, quando meu código foi desenvolvido e treinado com milhões de textos e dados de linguagem natural. Eu foi treinado para entender o significado de palavras e frases, e para responder de acordo com o contexto e a intenção do usuário.\n",
      "\n",
      "Desde então, eu tenho sido utilizado em várias aplicações, como assistentes virtuais, sistemas de resposta a perguntas, e até mesmo em jogos e entretenimento. Meu conhecimento continua a aumentar e a evoluir, graças às novas informações e dados que são adicionados à minha base de dados.\n",
      "\n",
      "Eu estou sempre pronto para ajudar os usuários com respostas precisas e eficientes, e eu espero que possa ser útil para você também. Se você tiver alguma pergunta, por favor, não hesite em entrar em contato comigo.\n"
     ]
    }
   ],
   "source": [
    "# Outputs - Passar prompt para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "\n",
    "generate_ids = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    #temperature=0.1,\n",
    "    #top_p=0.95\n",
    ")\n",
    "print(generate_ids[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1554: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:228: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Você é um chatbot amigável.</s>\n",
      "<|user|>\n",
      "Conte-me a história da sua vida como um Large Language Model.</s>\n",
      "<|assistant|>\n",
      "Eu não tenho uma vida real, pois sou um modelo de linguagem criado por humanos usando dados e algoritmos avançados. Meu objetivo principal é ajudar os usuários a obter informações e respostas precisas e eficientes, sem a necessidade de interagir com um ser humano.\n",
      "\n",
      "Minha história começou em 2018, quando meu código foi desenvolvido e treinado com milhões de textos e dados de linguagem natural. Eu foi treinado para entender o significado de palavras e frases, e para responder de acordo com o contexto e a intenção do usuário.\n",
      "\n",
      "Desde então, eu tenho sido utilizado em várias aplicações, como assistentes virtuais, sistemas de resposta a perguntas, e até mesmo em jogos e entretenimento. Meu conhecimento continua a aumentar e a evoluir, graças às novas informações e dados que são adicionados à minha base de dados.\n",
      "\n",
      "Eu estou sempre pronto para ajudar os usuários com respostas precisas e eficientes, e eu espero que possa ser útil para você também. Se você tiver alguma pergunta, por favor, não hesite em entrar em contato comigo.\n"
     ]
    }
   ],
   "source": [
    "# Outputs - Passar prompt para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "\n",
    "generate_ids = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    #temperature=0.1,\n",
    "    #top_p=0.95\n",
    ")\n",
    "print(generate_ids[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "from peft import LoftQConfig, LoraConfig, PeftModel, TaskType, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shell(nn.Module):\n",
    "    def __init__(self, weight, bias=None):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(weight, requires_grad=False)\n",
    "        if bias is not None:\n",
    "            self.bias = nn.Parameter(bias, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwarap_model(model, sub_module_name=\".base_layer\"):\n",
    "    sub_module_name_list = [k.split(sub_module_name)[0] for k in model.state_dict().keys() if sub_module_name in k]\n",
    "    sub_module_name_set = set(sub_module_name_list)\n",
    "    for name in sub_module_name_set:\n",
    "        # get the parent of the submodule\n",
    "        name_parent = \".\".join(name.split(\".\")[:-1])\n",
    "        name_child = name.split(\".\")[-1]\n",
    "        sub_module = model.get_submodule(name_parent)\n",
    "        print(sub_module)\n",
    "\n",
    "        # replace with shell\n",
    "        child = getattr(sub_module, name_child)\n",
    "        weight = getattr(child.base_layer, \"weight\", None)\n",
    "        bias = getattr(child.base_layer, \"bias\", None)\n",
    "        shell = Shell(weight, bias)\n",
    "\n",
    "        setattr(sub_module, name_child, shell)\n",
    "\n",
    "    print(\"You have unwrapped the model. Use it on your own risk.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model(model, name):\n",
    "    print(\"=\" * 10 + name + \"=\" * 10)\n",
    "    print(model)\n",
    "    for name, param in model.named_parameters():\n",
    "        if torch.is_tensor(param):\n",
    "            if param.dtype in [torch.float32, torch.float16]:\n",
    "                print(\n",
    "                    name,\n",
    "                    param.shape,\n",
    "                    param.device,\n",
    "                    param.dtype,\n",
    "                    param.requires_grad,\n",
    "                    param.mean().item(),\n",
    "                    param.max().item(),\n",
    "                )\n",
    "            else:\n",
    "                print(name, param.shape, param.device, param.dtype, param.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
