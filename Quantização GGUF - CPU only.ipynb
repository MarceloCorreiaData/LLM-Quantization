{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40cd04df",
   "metadata": {},
   "source": [
    "# Quantização GGUF com llama.cpp - cpu only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e5cac",
   "metadata": {},
   "source": [
    "## Login - Hugginface Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aef0390e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\marce\\anaconda3\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\marce\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\marce\\anaconda3\\lib\\site-packages (from huggingface_hub) (2023.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\marce\\anaconda3\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\marce\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\marce\\anaconda3\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\marce\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\marce\\anaconda3\\lib\\site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\marce\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\marce\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marce\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\marce\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marce\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2023.7.22)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b126753dec4b8483f7c86783f5a13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install huggingface_hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48e7cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aff461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive \n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25032db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# cache_dir = \"/content/drive/My Drive/huggingface_cache\"\n",
    "# os.makedirs(cache_dir, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -q einops\n",
    "# ! pip install sentencepiece\n",
    "# ! pip install gguf>=0.1.0\n",
    "# ! pip install protobuf>=4.21.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e9a947",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f90a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ [\"CUDA_VISIBLE_DEVICES\" ]=\"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02667495",
   "metadata": {},
   "source": [
    "## Model loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75788c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1b7f02e37d42b898d53f8068451d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36393bcaaf0544a78d42a0b795ddbee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba6f0ba177d4719bb9b39450e3a341f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ed44b3774f4111a520f6b076971d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c07f17f48c04eb89d0b514bdf4fe070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61fc0a50d1d345b88fa4145355e1e7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc3981e08404e66a0f3465ffcd11ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'mistralai/Mistral-7B-v0.1'\n",
    "\n",
    "model = AutoModelForCausalLM. from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='cpu', # ou 'auto', não importa\n",
    "    offload_folder='offload',\n",
    "    cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c1f23ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30626c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Capgemini\\LLMS\\Quantizacao\\llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp \n",
    "# %cd ../\n",
    "# Resposta: /workspace/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49522d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e22987",
   "metadata": {},
   "source": [
    "## Download dos tokenizer configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea509b61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded tokenizer_config.json\n",
      "Successfully downloaded tokenizer.model\n",
      "Successfully downloaded tokenizer.json\n",
      "Successfully downloaded special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def download_file_from_huggingface(model_name, filename, save_path, token):\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    url = f\"https://huggingface.co/{model_name}/resolve/main/{filename}\"\n",
    "    r = requests.get(url, headers=headers)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Failed to download {filename}. HTTP Status Code: {r.status_code}\") \n",
    "        return False\n",
    "    with open(os.path.join(save_path, filename), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    # HuggingFace token\n",
    "    huggingface_token = \"SEU_token\"\n",
    "\n",
    "    files_to_download = [\n",
    "        \"tokenizer_config.json\",\n",
    "        \"tokenizer.model\",\n",
    "        \"tokenizer.json\",\n",
    "        \"special_tokens_map.json\",\n",
    "    ]\n",
    "\n",
    "    # Nome/path do modelo\n",
    "    model_name = 'mistralai/Mistral-7B-v0.1'\n",
    "\n",
    "    # Criar diretório de modelos \n",
    "    save_path = \"./models\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    # Download files - usuário logado\n",
    "    for filename in files_to_download:\n",
    "        success = download_file_from_huggingface(model_name, filename, save_path, huggingface_token)\n",
    "        if success:\n",
    "            print(f\"Successfully downloaded {filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a7ac4",
   "metadata": {},
   "source": [
    "## Conversão para GGUF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c06a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tem de ser em Linux o WSL\n",
    "\n",
    "# Debian-based Linux padrão\n",
    "# !apt update -y\n",
    "# !apt install build-essential git cmake libopenblas-dev libeigen3-dev -y\n",
    "\n",
    "# Ou (CentOS 7 do BB)\n",
    "# !sudo yum groupinstall 'Development Tools' -y\n",
    "# !sudo yum install git cmake openblas-devel eigen3-devel -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf0c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make LLAMA_OPENBLAS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36e0af2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file models\\model.safetensors\n",
      "params = Params(n_vocab=32003, n_embd=2048, n_layer=22, n_ctx=2048, n_ff=5632, n_head=32, n_head_kv=4, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=WindowsPath('models'))\n",
      "32000 32000\n",
      "Vocab info: <VocabLoader with 32000 base tokens and 3 added tokens>\n",
      "Special vocab info: <SpecialVocab with 61249 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0, 'pad': 32000}, add special tokens unset>\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "lm_head.weight                                   -> output.weight                            | BF16   | [32003, 2048]\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | BF16   | [32003, 2048]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | BF16   | [2048]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | BF16   | [2048, 5632]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | BF16   | [5632, 2048]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | BF16   | [256, 2048]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | BF16   | [2048, 2048]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | BF16   | [2048, 2048]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | BF16   | [256, 2048]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | BF16   | [2048]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | BF16   | [2048, 5632]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | BF16   | [5632, 2048]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | BF16   | [2048]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | BF16   | [256, 2048]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | BF16   | [2048, 2048]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | BF16   | [2048, 2048]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | BF16   | [256, 2048]\n",
      "model.norm.weight                                -> output_norm.weight                       | BF16   | [2048]\n",
      "Writing models\\ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 61249 merge(s).\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting special token type pad to 32000\n",
      "[  1/201] Writing tensor output.weight                          | size  32003 x   2048  | type F16  | T+   0\n",
      "[  2/201] Writing tensor token_embd.weight                      | size  32003 x   2048  | type F16  | T+   0\n",
      "[  3/201] Writing tensor blk.0.attn_norm.weight                 | size   2048           | type F32  | T+   1\n",
      "[  4/201] Writing tensor blk.0.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   1\n",
      "[  5/201] Writing tensor blk.0.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   1\n",
      "[  6/201] Writing tensor blk.0.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   1\n",
      "[  7/201] Writing tensor blk.0.ffn_norm.weight                  | size   2048           | type F32  | T+   1\n",
      "[  8/201] Writing tensor blk.0.attn_k.weight                    | size    256 x   2048  | type F16  | T+   1\n",
      "[  9/201] Writing tensor blk.0.attn_output.weight               | size   2048 x   2048  | type F16  | T+   1\n",
      "[ 10/201] Writing tensor blk.0.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   1\n",
      "[ 11/201] Writing tensor blk.0.attn_v.weight                    | size    256 x   2048  | type F16  | T+   1\n",
      "[ 12/201] Writing tensor blk.1.attn_norm.weight                 | size   2048           | type F32  | T+   1\n",
      "[ 13/201] Writing tensor blk.1.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   1\n",
      "[ 14/201] Writing tensor blk.1.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   1\n",
      "[ 15/201] Writing tensor blk.1.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   1\n",
      "[ 16/201] Writing tensor blk.1.ffn_norm.weight                  | size   2048           | type F32  | T+   1\n",
      "[ 17/201] Writing tensor blk.1.attn_k.weight                    | size    256 x   2048  | type F16  | T+   1\n",
      "[ 18/201] Writing tensor blk.1.attn_output.weight               | size   2048 x   2048  | type F16  | T+   1\n",
      "[ 19/201] Writing tensor blk.1.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   1\n",
      "[ 20/201] Writing tensor blk.1.attn_v.weight                    | size    256 x   2048  | type F16  | T+   1\n",
      "[ 21/201] Writing tensor blk.10.attn_norm.weight                | size   2048           | type F32  | T+   1\n",
      "[ 22/201] Writing tensor blk.10.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   1\n",
      "[ 23/201] Writing tensor blk.10.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   1\n",
      "[ 24/201] Writing tensor blk.10.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   1\n",
      "[ 25/201] Writing tensor blk.10.ffn_norm.weight                 | size   2048           | type F32  | T+   1\n",
      "[ 26/201] Writing tensor blk.10.attn_k.weight                   | size    256 x   2048  | type F16  | T+   1\n",
      "[ 27/201] Writing tensor blk.10.attn_output.weight              | size   2048 x   2048  | type F16  | T+   1\n",
      "[ 28/201] Writing tensor blk.10.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   1\n",
      "[ 29/201] Writing tensor blk.10.attn_v.weight                   | size    256 x   2048  | type F16  | T+   1\n",
      "[ 30/201] Writing tensor blk.11.attn_norm.weight                | size   2048           | type F32  | T+   1\n",
      "[ 31/201] Writing tensor blk.11.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   1\n",
      "[ 32/201] Writing tensor blk.11.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   1\n",
      "[ 33/201] Writing tensor blk.11.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   1\n",
      "[ 34/201] Writing tensor blk.11.ffn_norm.weight                 | size   2048           | type F32  | T+   1\n",
      "[ 35/201] Writing tensor blk.11.attn_k.weight                   | size    256 x   2048  | type F16  | T+   1\n",
      "[ 36/201] Writing tensor blk.11.attn_output.weight              | size   2048 x   2048  | type F16  | T+   1\n",
      "[ 37/201] Writing tensor blk.11.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   1\n",
      "[ 38/201] Writing tensor blk.11.attn_v.weight                   | size    256 x   2048  | type F16  | T+   1\n",
      "[ 39/201] Writing tensor blk.12.attn_norm.weight                | size   2048           | type F32  | T+   1\n",
      "[ 40/201] Writing tensor blk.12.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   1\n",
      "[ 41/201] Writing tensor blk.12.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   1\n",
      "[ 42/201] Writing tensor blk.12.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   1\n",
      "[ 43/201] Writing tensor blk.12.ffn_norm.weight                 | size   2048           | type F32  | T+   1\n",
      "[ 44/201] Writing tensor blk.12.attn_k.weight                   | size    256 x   2048  | type F16  | T+   1\n",
      "[ 45/201] Writing tensor blk.12.attn_output.weight              | size   2048 x   2048  | type F16  | T+   1\n",
      "[ 46/201] Writing tensor blk.12.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   1\n",
      "[ 47/201] Writing tensor blk.12.attn_v.weight                   | size    256 x   2048  | type F16  | T+   1\n",
      "[ 48/201] Writing tensor blk.13.attn_norm.weight                | size   2048           | type F32  | T+   1\n",
      "[ 49/201] Writing tensor blk.13.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   1\n",
      "[ 50/201] Writing tensor blk.13.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   1\n",
      "[ 51/201] Writing tensor blk.13.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   1\n",
      "[ 52/201] Writing tensor blk.13.ffn_norm.weight                 | size   2048           | type F32  | T+   2\n",
      "[ 53/201] Writing tensor blk.13.attn_k.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 54/201] Writing tensor blk.13.attn_output.weight              | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 55/201] Writing tensor blk.13.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 56/201] Writing tensor blk.13.attn_v.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 57/201] Writing tensor blk.14.attn_norm.weight                | size   2048           | type F32  | T+   2\n",
      "[ 58/201] Writing tensor blk.14.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   2\n",
      "[ 59/201] Writing tensor blk.14.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   2\n",
      "[ 60/201] Writing tensor blk.14.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   2\n",
      "[ 61/201] Writing tensor blk.14.ffn_norm.weight                 | size   2048           | type F32  | T+   2\n",
      "[ 62/201] Writing tensor blk.14.attn_k.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 63/201] Writing tensor blk.14.attn_output.weight              | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 64/201] Writing tensor blk.14.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 65/201] Writing tensor blk.14.attn_v.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 66/201] Writing tensor blk.15.attn_norm.weight                | size   2048           | type F32  | T+   2\n",
      "[ 67/201] Writing tensor blk.15.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   2\n",
      "[ 68/201] Writing tensor blk.15.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   2\n",
      "[ 69/201] Writing tensor blk.15.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   2\n",
      "[ 70/201] Writing tensor blk.15.ffn_norm.weight                 | size   2048           | type F32  | T+   2\n",
      "[ 71/201] Writing tensor blk.15.attn_k.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 72/201] Writing tensor blk.15.attn_output.weight              | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 73/201] Writing tensor blk.15.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 74/201] Writing tensor blk.15.attn_v.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 75/201] Writing tensor blk.16.attn_norm.weight                | size   2048           | type F32  | T+   2\n",
      "[ 76/201] Writing tensor blk.16.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   2\n",
      "[ 77/201] Writing tensor blk.16.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   2\n",
      "[ 78/201] Writing tensor blk.16.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   2\n",
      "[ 79/201] Writing tensor blk.16.ffn_norm.weight                 | size   2048           | type F32  | T+   2\n",
      "[ 80/201] Writing tensor blk.16.attn_k.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 81/201] Writing tensor blk.16.attn_output.weight              | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 82/201] Writing tensor blk.16.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 83/201] Writing tensor blk.16.attn_v.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 84/201] Writing tensor blk.17.attn_norm.weight                | size   2048           | type F32  | T+   2\n",
      "[ 85/201] Writing tensor blk.17.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   2\n",
      "[ 86/201] Writing tensor blk.17.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   2\n",
      "[ 87/201] Writing tensor blk.17.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   2\n",
      "[ 88/201] Writing tensor blk.17.ffn_norm.weight                 | size   2048           | type F32  | T+   2\n",
      "[ 89/201] Writing tensor blk.17.attn_k.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 90/201] Writing tensor blk.17.attn_output.weight              | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 91/201] Writing tensor blk.17.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   2\n",
      "[ 92/201] Writing tensor blk.17.attn_v.weight                   | size    256 x   2048  | type F16  | T+   2\n",
      "[ 93/201] Writing tensor blk.18.attn_norm.weight                | size   2048           | type F32  | T+   2\n",
      "[ 94/201] Writing tensor blk.18.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   2\n",
      "[ 95/201] Writing tensor blk.18.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   2\n",
      "[ 96/201] Writing tensor blk.18.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   2\n",
      "[ 97/201] Writing tensor blk.18.ffn_norm.weight                 | size   2048           | type F32  | T+   3\n",
      "[ 98/201] Writing tensor blk.18.attn_k.weight                   | size    256 x   2048  | type F16  | T+   3\n",
      "[ 99/201] Writing tensor blk.18.attn_output.weight              | size   2048 x   2048  | type F16  | T+   3\n",
      "[100/201] Writing tensor blk.18.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   3\n",
      "[101/201] Writing tensor blk.18.attn_v.weight                   | size    256 x   2048  | type F16  | T+   3\n",
      "[102/201] Writing tensor blk.19.attn_norm.weight                | size   2048           | type F32  | T+   3\n",
      "[103/201] Writing tensor blk.19.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   3\n",
      "[104/201] Writing tensor blk.19.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   3\n",
      "[105/201] Writing tensor blk.19.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   3\n",
      "[106/201] Writing tensor blk.19.ffn_norm.weight                 | size   2048           | type F32  | T+   3\n",
      "[107/201] Writing tensor blk.19.attn_k.weight                   | size    256 x   2048  | type F16  | T+   3\n",
      "[108/201] Writing tensor blk.19.attn_output.weight              | size   2048 x   2048  | type F16  | T+   3\n",
      "[109/201] Writing tensor blk.19.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   3\n",
      "[110/201] Writing tensor blk.19.attn_v.weight                   | size    256 x   2048  | type F16  | T+   3\n",
      "[111/201] Writing tensor blk.2.attn_norm.weight                 | size   2048           | type F32  | T+   3\n",
      "[112/201] Writing tensor blk.2.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   3\n",
      "[113/201] Writing tensor blk.2.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   3\n",
      "[114/201] Writing tensor blk.2.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   3\n",
      "[115/201] Writing tensor blk.2.ffn_norm.weight                  | size   2048           | type F32  | T+   3\n",
      "[116/201] Writing tensor blk.2.attn_k.weight                    | size    256 x   2048  | type F16  | T+   3\n",
      "[117/201] Writing tensor blk.2.attn_output.weight               | size   2048 x   2048  | type F16  | T+   3\n",
      "[118/201] Writing tensor blk.2.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   3\n",
      "[119/201] Writing tensor blk.2.attn_v.weight                    | size    256 x   2048  | type F16  | T+   3\n",
      "[120/201] Writing tensor blk.20.attn_norm.weight                | size   2048           | type F32  | T+   3\n",
      "[121/201] Writing tensor blk.20.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   3\n",
      "[122/201] Writing tensor blk.20.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   3\n",
      "[123/201] Writing tensor blk.20.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   3\n",
      "[124/201] Writing tensor blk.20.ffn_norm.weight                 | size   2048           | type F32  | T+   3\n",
      "[125/201] Writing tensor blk.20.attn_k.weight                   | size    256 x   2048  | type F16  | T+   3\n",
      "[126/201] Writing tensor blk.20.attn_output.weight              | size   2048 x   2048  | type F16  | T+   3\n",
      "[127/201] Writing tensor blk.20.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   3\n",
      "[128/201] Writing tensor blk.20.attn_v.weight                   | size    256 x   2048  | type F16  | T+   3\n",
      "[129/201] Writing tensor blk.21.attn_norm.weight                | size   2048           | type F32  | T+   3\n",
      "[130/201] Writing tensor blk.21.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   3\n",
      "[131/201] Writing tensor blk.21.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   3\n",
      "[132/201] Writing tensor blk.21.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   3\n",
      "[133/201] Writing tensor blk.21.ffn_norm.weight                 | size   2048           | type F32  | T+   3\n",
      "[134/201] Writing tensor blk.21.attn_k.weight                   | size    256 x   2048  | type F16  | T+   3\n",
      "[135/201] Writing tensor blk.21.attn_output.weight              | size   2048 x   2048  | type F16  | T+   3\n",
      "[136/201] Writing tensor blk.21.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   3\n",
      "[137/201] Writing tensor blk.21.attn_v.weight                   | size    256 x   2048  | type F16  | T+   3\n",
      "[138/201] Writing tensor blk.3.attn_norm.weight                 | size   2048           | type F32  | T+   3\n",
      "[139/201] Writing tensor blk.3.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   3\n",
      "[140/201] Writing tensor blk.3.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   3\n",
      "[141/201] Writing tensor blk.3.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   4\n",
      "[142/201] Writing tensor blk.3.ffn_norm.weight                  | size   2048           | type F32  | T+   4\n",
      "[143/201] Writing tensor blk.3.attn_k.weight                    | size    256 x   2048  | type F16  | T+   4\n",
      "[144/201] Writing tensor blk.3.attn_output.weight               | size   2048 x   2048  | type F16  | T+   4\n",
      "[145/201] Writing tensor blk.3.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   4\n",
      "[146/201] Writing tensor blk.3.attn_v.weight                    | size    256 x   2048  | type F16  | T+   4\n",
      "[147/201] Writing tensor blk.4.attn_norm.weight                 | size   2048           | type F32  | T+   4\n",
      "[148/201] Writing tensor blk.4.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   4\n",
      "[149/201] Writing tensor blk.4.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   4\n",
      "[150/201] Writing tensor blk.4.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   4\n",
      "[151/201] Writing tensor blk.4.ffn_norm.weight                  | size   2048           | type F32  | T+   4\n",
      "[152/201] Writing tensor blk.4.attn_k.weight                    | size    256 x   2048  | type F16  | T+   4\n",
      "[153/201] Writing tensor blk.4.attn_output.weight               | size   2048 x   2048  | type F16  | T+   4\n",
      "[154/201] Writing tensor blk.4.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   4\n",
      "[155/201] Writing tensor blk.4.attn_v.weight                    | size    256 x   2048  | type F16  | T+   4\n",
      "[156/201] Writing tensor blk.5.attn_norm.weight                 | size   2048           | type F32  | T+   4\n",
      "[157/201] Writing tensor blk.5.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   4\n",
      "[158/201] Writing tensor blk.5.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   4\n",
      "[159/201] Writing tensor blk.5.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   4\n",
      "[160/201] Writing tensor blk.5.ffn_norm.weight                  | size   2048           | type F32  | T+   4\n",
      "[161/201] Writing tensor blk.5.attn_k.weight                    | size    256 x   2048  | type F16  | T+   4\n",
      "[162/201] Writing tensor blk.5.attn_output.weight               | size   2048 x   2048  | type F16  | T+   4\n",
      "[163/201] Writing tensor blk.5.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   4\n",
      "[164/201] Writing tensor blk.5.attn_v.weight                    | size    256 x   2048  | type F16  | T+   4\n",
      "[165/201] Writing tensor blk.6.attn_norm.weight                 | size   2048           | type F32  | T+   4\n",
      "[166/201] Writing tensor blk.6.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   4\n",
      "[167/201] Writing tensor blk.6.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   4\n",
      "[168/201] Writing tensor blk.6.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   4\n",
      "[169/201] Writing tensor blk.6.ffn_norm.weight                  | size   2048           | type F32  | T+   4\n",
      "[170/201] Writing tensor blk.6.attn_k.weight                    | size    256 x   2048  | type F16  | T+   4\n",
      "[171/201] Writing tensor blk.6.attn_output.weight               | size   2048 x   2048  | type F16  | T+   4\n",
      "[172/201] Writing tensor blk.6.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   4\n",
      "[173/201] Writing tensor blk.6.attn_v.weight                    | size    256 x   2048  | type F16  | T+   4\n",
      "[174/201] Writing tensor blk.7.attn_norm.weight                 | size   2048           | type F32  | T+   4\n",
      "[175/201] Writing tensor blk.7.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   4\n",
      "[176/201] Writing tensor blk.7.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   4\n",
      "[177/201] Writing tensor blk.7.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   4\n",
      "[178/201] Writing tensor blk.7.ffn_norm.weight                  | size   2048           | type F32  | T+   4\n",
      "[179/201] Writing tensor blk.7.attn_k.weight                    | size    256 x   2048  | type F16  | T+   4\n",
      "[180/201] Writing tensor blk.7.attn_output.weight               | size   2048 x   2048  | type F16  | T+   4\n",
      "[181/201] Writing tensor blk.7.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   4\n",
      "[182/201] Writing tensor blk.7.attn_v.weight                    | size    256 x   2048  | type F16  | T+   4\n",
      "[183/201] Writing tensor blk.8.attn_norm.weight                 | size   2048           | type F32  | T+   4\n",
      "[184/201] Writing tensor blk.8.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   4\n",
      "[185/201] Writing tensor blk.8.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   5\n",
      "[186/201] Writing tensor blk.8.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   5\n",
      "[187/201] Writing tensor blk.8.ffn_norm.weight                  | size   2048           | type F32  | T+   5\n",
      "[188/201] Writing tensor blk.8.attn_k.weight                    | size    256 x   2048  | type F16  | T+   5\n",
      "[189/201] Writing tensor blk.8.attn_output.weight               | size   2048 x   2048  | type F16  | T+   5\n",
      "[190/201] Writing tensor blk.8.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   5\n",
      "[191/201] Writing tensor blk.8.attn_v.weight                    | size    256 x   2048  | type F16  | T+   5\n",
      "[192/201] Writing tensor blk.9.attn_norm.weight                 | size   2048           | type F32  | T+   5\n",
      "[193/201] Writing tensor blk.9.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   5\n",
      "[194/201] Writing tensor blk.9.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   5\n",
      "[195/201] Writing tensor blk.9.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   5\n",
      "[196/201] Writing tensor blk.9.ffn_norm.weight                  | size   2048           | type F32  | T+   5\n",
      "[197/201] Writing tensor blk.9.attn_k.weight                    | size    256 x   2048  | type F16  | T+   5\n",
      "[198/201] Writing tensor blk.9.attn_output.weight               | size   2048 x   2048  | type F16  | T+   5\n",
      "[199/201] Writing tensor blk.9.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   5\n",
      "[200/201] Writing tensor blk.9.attn_v.weight                    | size    256 x   2048  | type F16  | T+   5\n",
      "[201/201] Writing tensor output_norm.weight                     | size   2048           | type F32  | T+   5\n",
      "Wrote models\\ggml-model-f16.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "!python convert.py models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03241fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GGML está DEPRECATED!\n",
    "#!./quantize ./ggml-model-f16.bin ./ggml-model-q4_K_M.bin q4_K_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fddac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string pelo caracter '/'\n",
    "parts = model_name.split('/')\n",
    "\n",
    "# Pegar a segund aparte (index 1) \n",
    "model_name_pure = parts[1]\n",
    "\n",
    "quant_type = \"Q4_K\"\n",
    "quantized_model = f'models/{model_name_pure}.{quant_type}.gguf'\n",
    "print(f'Preparando {quantized_model} com {quant_type} quantização.')\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Build o command como uma lista argumentos\n",
    "command = [\"./quantize\", \"models/ggml-model-f16.gguf\", quantized_model, quant_type]\n",
    "\n",
    "# Executar o command\n",
    "subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae055ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ddb15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75007e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107e7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
